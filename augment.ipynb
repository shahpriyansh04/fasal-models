{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in augmented dataset: 14982\n",
      "\n",
      "Class distribution in augmented dataset:\n",
      "label\n",
      "cotton         681\n",
      "apple          681\n",
      "mungbean       681\n",
      "pomegranate    681\n",
      "rice           681\n",
      "mango          681\n",
      "muskmelon      681\n",
      "pigeonpeas     681\n",
      "orange         681\n",
      "grapes         681\n",
      "watermelon     681\n",
      "banana         681\n",
      "maize          681\n",
      "blackgram      681\n",
      "coffee         681\n",
      "chickpea       681\n",
      "kidneybeans    681\n",
      "papaya         681\n",
      "mothbeans      681\n",
      "coconut        681\n",
      "jute           681\n",
      "lentil         681\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset size: 14982\n",
      "\n",
      "Final class distribution:\n",
      "label\n",
      "cotton         681\n",
      "apple          681\n",
      "mungbean       681\n",
      "pomegranate    681\n",
      "rice           681\n",
      "mango          681\n",
      "muskmelon      681\n",
      "pigeonpeas     681\n",
      "orange         681\n",
      "grapes         681\n",
      "watermelon     681\n",
      "banana         681\n",
      "maize          681\n",
      "blackgram      681\n",
      "coffee         681\n",
      "chickpea       681\n",
      "kidneybeans    681\n",
      "papaya         681\n",
      "mothbeans      681\n",
      "coconut        681\n",
      "jute           681\n",
      "lentil         681\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_noise(data, noise_level=0.05):\n",
    "    noisy_data = data.copy()\n",
    "    for column in noisy_data.columns[:-1]:  \n",
    "        if noisy_data[column].dtype != 'object':\n",
    "            noise = np.random.normal(0, noise_level, size=noisy_data[column].shape)\n",
    "            noisy_data[column] += noise * noisy_data[column]\n",
    "    return noisy_data\n",
    "\n",
    "def scale_features(data, scale_range=(0.9, 1.1)):\n",
    "    scaled_data = data.copy()\n",
    "    for column in scaled_data.columns[:-1]:  \n",
    "        if scaled_data[column].dtype != 'object':\n",
    "            scale_factor = np.random.uniform(*scale_range, size=scaled_data[column].shape)\n",
    "            scaled_data[column] *= scale_factor\n",
    "    return scaled_data\n",
    "\n",
    "def generate_synthetic_data(data, samples_per_class):\n",
    "    synthetic_data = pd.DataFrame()\n",
    "    for crop in data['label'].unique():\n",
    "        crop_data = data[data['label'] == crop]\n",
    "        for _ in range(samples_per_class - len(crop_data)):\n",
    "            noisy_data = add_noise(crop_data.sample(1))\n",
    "            scaled_data = scale_features(noisy_data)\n",
    "            synthetic_data = pd.concat([synthetic_data, scaled_data], ignore_index=True)\n",
    "    return synthetic_data\n",
    "\n",
    "file_path = '1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "num_classes = len(data['label'].unique())\n",
    "samples_per_class = 15000 // num_classes\n",
    "\n",
    "synthetic_data = generate_synthetic_data(data, samples_per_class)\n",
    "\n",
    "augmented_data = pd.concat([data, synthetic_data], ignore_index=True)\n",
    "\n",
    "augmented_data = augmented_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "augmented_data.to_csv('2.csv', index=False)\n",
    "\n",
    "print(\"Total samples in augmented dataset:\", len(augmented_data))\n",
    "print(\"\\nClass distribution in augmented dataset:\")\n",
    "print(augmented_data['label'].value_counts())\n",
    "\n",
    "if len(augmented_data) > 20000:\n",
    "    augmented_data = augmented_data.sample(n=20000, random_state=42)\n",
    "elif len(augmented_data) < 10000:\n",
    "    additional_samples_needed = 10000 - len(augmented_data)\n",
    "    additional_samples = augmented_data.sample(n=additional_samples_needed, replace=True, random_state=42)\n",
    "    augmented_data = pd.concat([augmented_data, additional_samples], ignore_index=True)\n",
    "\n",
    "augmented_data.to_csv('2.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal dataset size:\", len(augmented_data))\n",
    "print(\"\\nFinal class distribution:\")\n",
    "print(augmented_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
